# NER Token Classification (TYPE / BRAND / VOLUME / PERCENT)

Полный README к проекту: обучение и свип гиперпараметров токен-классификатора на базе `bert-base-multilingual-cased` для NER над товарными описаниями. Особый акцент — на улучшение качества по редким классам `VOLUME` и `PERCENT`.

---

## Содержание
- [Идея и цели](#идея-и-цели)  
- [Данные и происхождение “clean” выборки](#данные-и-происхождение-clean-выборки)  
- [Подготовка данных и фильтрация VOLUME/PERCENT](#подготовка-данных-и-фильтрация-volumepercent)  
- [Разбиение на трейн/валидацию (group split)](#разбиение-на-трейнвалидацию-group-split)  
- [BIO-разметка и токенизация](#bio-разметка-и-токенизация)  
- [Файнтюнинг (fine-tuning)](#файнтюнинг-fine-tuning)  
- [Взвешивание классов и Label Smoothing](#взвешивание-классов-и-label-smoothing)  
- [Метрики](#метрики)  
- [Тренировка одной модели](#тренировка-одной-модели)  
- [Свип гиперпараметров + автосабмиты](#свип-гиперпараметров--автосабмиты)  
- [Постпроцессинг B→I цепочек](#постпроцессинг-bi-цепочек)  
- [Быстрый инференс](#быстрый-инференс)  
- [Требования и установка](#требования-и-установка)  
- [Структура репозитория](#структура-репозитория)  
- [Репродьюсибилити](#репродьюсибилити)  
- [Лицензия данных и атрибуция](#лицензия-данных-и-атрибуция)  
- [Идеи для дальнейшей работы](#идеи-для-дальнейшей-работы)

---

## Идея и цели
1. Смешать «шумные» разметки (`train.csv`) и «чистые» примеры (`train-clean-volperc.csv`) так, чтобы усилить сигнал по `VOLUME`/`PERCENT` и не потерять общий контекст.  
2. Снизить перекос в сторону частых классов через **sample-wise class weights** и **label smoothing**.  
3. Контролировать состав валидации (≈80% noisy / 20% clean) и выбирать лучшую модель по **Macro-F1 на спанах**.

---

## Данные и происхождение “clean” выборки
**Источник “clean” данных:** Open Food Facts (OFF). Из выгрузки карточек продуктов использованы релевантные колонки (типично: `product_name`, `brands`, `quantity`, иногда `packaging`, `ingredients_text`, `categories` и др.).

**Как размечали:**
- **VOLUME.** `quantity`/смежные поля содержат фасовку/объём: `500 g`, `0.75 L`, `12 x 33 cl`, `250ml`, `1kg`, `2×200 g`, `3x1L` и т.п. Эти выражения автоматически находились регулярками и размечались как `B- / I-VOLUME` (в символьных оффсетах исходного текста).
- **PERCENT (синтетика).** Реальные проценты встречаются реже, поэтому добавлены синтетические фразы по популярным категориям, где проценты логичны:  
  - шоколад/какао: `cacao 70%`, `85% cocoa`;  
  - молочка/йогурты: `3.5% fat`, `1,8% жирности`;  
  - белок/сыры: `12% protein`, `45% fat in dry matter`.  
  Маркетинговые проценты (например, «скидка 20%») **не размечались** как `PERCENT`.

**Важно:** укажите у себя дату выгрузки OFF и ссылку на релиз/снэпшот — это нужно для корректной атрибуции (см. раздел «Лицензия данных»).

---

## Подготовка данных и фильтрация VOLUME/PERCENT
Скрипт формирует “чистый” набор по `VOLUME/PERCENT` из исходного `train-clean.csv` и сохраняет в `train-clean-volperc.csv`.

---

## Разбиение на трейн/валидацию (group split)
- Для noisy и clean делаем отдельные **GroupShuffleSplit** по группам, чтобы похожие тексты не утекали между train/val.  
- Доли по умолчанию: `VAL_NOISY_FRAC = 0.12`, `VAL_CLEAN_FRAC = 0.05`.  
- При `ENFORCE_VAL_MIX=True` приводим итоговую валидацию к ~`VAL_TARGET_NOISY_RATIO = 0.8` (≈80% noisy).

---

## BIO-разметка и токенизация
- Символьные спаны конвертируются в BIO-теги **на уровне слов**: первый покрытый спаном токен — `B-<TYPE>`, далее — `I-<TYPE>`.  
- Токенизация — `AutoTokenizer.from_pretrained("bert-base-multilingual-cased", use_fast=True)` с `is_split_into_words=True`.  
- Сабтокены внутри одного слова получают `-100` (игнор в лоссе).  
- `MAX_LEN` по умолчанию `128`.

---

## Файнтюнинг (fine-tuning)
- Базовая модель: `bert-base-multilingual-cased`.  
- Head: linear, num_labels = len(BIO_LABELS).  
- Bias инициализируется лог-частотами классов по трейну.  
- EarlyStopping patience=4 по Macro-F1.  
- LR=1.5e-5, EPOCHS=6, BATCH=16, MAX_LEN=128, SEED=42.

---

## Взвешивание классов и Label Smoothing
- Sample-wise class weights по наличию класса в сэмплах, клип [0.5; 3.0].  
- Label smoothing = 0.05.  

---

## Метрики
- Token-level (seqeval): Precision / Recall / F1 / Accuracy.  
- Entity-level STRICT: TP/FP/FN, Precision/Recall/F1 per type, Macro-F1.  

---

## Тренировка одной модели
```bash
python train.py
```
Чекпойнт → `./bert_tokcls_noise_fix`.

---

## Свип гиперпараметров + автосабмиты
Файл: `sweep_train_and_submit.py`  
- тренировка по SEARCH_SPACE,  
- сохранение в ./runs,  
- сабмиты в ./submissions,  
- лог tune_log.csv.  

---

## Постпроцессинг B→I цепочек
Функция fix_bi_chains конвертирует лишние B- в I- внутри одного спана.

---

## Быстрый инференс
`infer_on_text(text)` → список (слово, BIO-метка).  
`sweep_train_and_submit.py` делает батчевый инференс для submission.csv.

---

## Требования и установка
```
pip install torch transformers datasets scikit-learn pandas numpy
```

---

## Структура репозитория
```
train.csv
train-clean.csv
train-clean-volperc.csv
submission.csv
train.ipynb
runs/
submissions/
README.md
```

---

## Репродьюсибилити
- Фиксируем сиды.  
- Group split по md5 нормализованного текста.  

---

## Лицензия данных и атрибуция
Open Food Facts → ODbL 1.0.  
Укажите OFF, дату выгрузки, ссылку.  

---

## Идеи для дальнейшей работы
- Layer-wise LR decay.  
- Gradual unfreezing.  
- Domain Adaptive Pretraining.  
- MAX_LEN=256.  
- Усиление loss по VOLUME/PERCENT.
