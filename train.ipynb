{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36c4dd7-a9ad-4fb3-88ba-11e651de7ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 15:17:34.989322: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-02 15:17:34.997462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759411055.006926  697725 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759411055.010066  697725 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759411055.018446  697725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759411055.018460  697725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759411055.018461  697725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759411055.018462  697725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-02 15:17:35.020964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== RUN 1/1: {'LR': 1.65e-05, 'BATCH': 48, 'EPOCHS': 5, 'LABEL_SMOOTHING': 0.03, 'DROPOUT_H': 0.1, 'DROPOUT_A': 0.1, 'MAX_LEN': 128, 'GRAD_NORM_CLIP': 0.8} ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfa7c6ecdee48a49168c002a7c68fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6872e269270d43c0819571b6cf51a455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_697725/1058687982.py:262: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSmoothedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3865' max='3865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3865/3865 03:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision Token</th>\n",
       "      <th>Recall Token</th>\n",
       "      <th>F1 Token</th>\n",
       "      <th>Accuracy Token</th>\n",
       "      <th>Macro F1 Entity</th>\n",
       "      <th>Entity Report Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.481256</td>\n",
       "      <td>0.923027</td>\n",
       "      <td>0.940667</td>\n",
       "      <td>0.931763</td>\n",
       "      <td>0.925317</td>\n",
       "      <td>0.932993</td>\n",
       "      <td>Entity-level (STRICT spans):\n",
       "Type       TP   FP   FN   Prec     Rec      F1\n",
       "TYPE       3274 180  151   0.948   0.956   0.952\n",
       "BRAND      838  168  110   0.833   0.884   0.858\n",
       "VOLUME     76   5    4     0.938   0.950   0.944\n",
       "PERCENT    45   0    2     1.000   0.957   0.978\n",
       "\n",
       "Macro-F1 (over TYPE, BRAND, VOLUME, PERCENT): 0.9330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.330400</td>\n",
       "      <td>0.435670</td>\n",
       "      <td>0.947045</td>\n",
       "      <td>0.957778</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.943468</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>Entity-level (STRICT spans):\n",
       "Type       TP   FP   FN   Prec     Rec      F1\n",
       "TYPE       3316 136  109   0.961   0.968   0.964\n",
       "BRAND      870  103  78    0.894   0.918   0.906\n",
       "VOLUME     78   2    2     0.975   0.975   0.975\n",
       "PERCENT    46   0    1     1.000   0.979   0.989\n",
       "\n",
       "Macro-F1 (over TYPE, BRAND, VOLUME, PERCENT): 0.9586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.454476</td>\n",
       "      <td>0.948348</td>\n",
       "      <td>0.962889</td>\n",
       "      <td>0.955563</td>\n",
       "      <td>0.947627</td>\n",
       "      <td>0.960320</td>\n",
       "      <td>Entity-level (STRICT spans):\n",
       "Type       TP   FP   FN   Prec     Rec      F1\n",
       "TYPE       3360 168  65    0.952   0.981   0.966\n",
       "BRAND      848  64   100   0.930   0.895   0.912\n",
       "VOLUME     78   4    2     0.951   0.975   0.963\n",
       "PERCENT    47   0    0     1.000   1.000   1.000\n",
       "\n",
       "Macro-F1 (over TYPE, BRAND, VOLUME, PERCENT): 0.9603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.444901</td>\n",
       "      <td>0.949517</td>\n",
       "      <td>0.961333</td>\n",
       "      <td>0.955389</td>\n",
       "      <td>0.946871</td>\n",
       "      <td>0.963488</td>\n",
       "      <td>Entity-level (STRICT spans):\n",
       "Type       TP   FP   FN   Prec     Rec      F1\n",
       "TYPE       3323 130  102   0.962   0.970   0.966\n",
       "BRAND      878  98   70    0.900   0.926   0.913\n",
       "VOLUME     78   2    2     0.975   0.975   0.975\n",
       "PERCENT    47   0    0     1.000   1.000   1.000\n",
       "\n",
       "Macro-F1 (over TYPE, BRAND, VOLUME, PERCENT): 0.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.457028</td>\n",
       "      <td>0.951578</td>\n",
       "      <td>0.965111</td>\n",
       "      <td>0.958297</td>\n",
       "      <td>0.950085</td>\n",
       "      <td>0.964208</td>\n",
       "      <td>Entity-level (STRICT spans):\n",
       "Type       TP   FP   FN   Prec     Rec      F1\n",
       "TYPE       3345 140  80    0.960   0.977   0.968\n",
       "BRAND      872  77   76    0.919   0.920   0.919\n",
       "VOLUME     79   4    1     0.952   0.988   0.969\n",
       "PERCENT    47   0    0     1.000   1.000   1.000\n",
       "\n",
       "Macro-F1 (over TYPE, BRAND, VOLUME, PERCENT): 0.9642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved submission → ./submissions-final/submission_20251002-152119_lr1p65e-05_bs48_ep5_ls0p03_do0p1-0p1_ml128_seed42_macro0p9642_f1tok0p9583.csv\n",
      "✔ Logged to        → ./submissions-final/tune_log.csv\n"
     ]
    }
   ],
   "source": [
    "# sweep_train_and_submit.py\n",
    "# Hyperparameter sweep for your NER trainer + per-run submission files and a master CSV log.\n",
    "\n",
    "import os, sys, ast, json, math, inspect, hashlib, random, time, gc\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification, TrainingArguments, Trainer,\n",
    "    EarlyStoppingCallback, TrainerCallback\n",
    ")\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# ======================== CONSTANTS (can be overridden per-run) ========================\n",
    "DEFAULTS = dict(\n",
    "    NOISY_PATH  = \"train.csv\",\n",
    "    CLEAN_PATH  = \"train-clean-volperc.csv\",\n",
    "    MODEL_NAME  = \"bert-base-multilingual-cased\",\n",
    "\n",
    "    VAL_NOISY_FRAC = 0.12,\n",
    "    VAL_CLEAN_FRAC = 0.05,\n",
    "    ENFORCE_VAL_MIX = True,\n",
    "    VAL_TARGET_NOISY_RATIO = 0.8,   # ≈80% noisy in validation\n",
    "\n",
    "    ENTITY_TYPES = [\"TYPE\", \"BRAND\", \"VOLUME\", \"PERCENT\"],\n",
    "    MAX_LEN   = 128,\n",
    "    EPOCHS    = 6,\n",
    "    BATCH     = 16,\n",
    "    LR        = 1.5e-5,\n",
    "    SEED      = 42,\n",
    "    LABEL_SMOOTHING = 0.05,\n",
    "    DROPOUT_H = 0.2,\n",
    "    DROPOUT_A = 0.2,\n",
    "    MAX_WEIGHT = 3.0,\n",
    "    MIN_WEIGHT = 0.5,\n",
    "    GRAD_NORM_CLIP = 1.0,\n",
    "\n",
    "    OUTPUT_ROOT = \"./runs\",           # each run gets its own subdir here\n",
    "    SUBMIT_DIR  = \"./submissions-final\",    # all submission files collected here\n",
    "    SUBMISSION_PATH = \"submission.csv\",\n",
    "    BATCH_INFER = 64,\n",
    ")\n",
    "\n",
    "BIO_LABELS_CACHE = {}\n",
    "\n",
    "# ======================== UTILS ========================\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def safe_mkdir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def now_tag():\n",
    "    return datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def label_set(entity_types: List[str]) -> List[str]:\n",
    "    # cache per entity_types signature\n",
    "    key = tuple(entity_types)\n",
    "    if key not in BIO_LABELS_CACHE:\n",
    "        BIO_LABELS_CACHE[key] = [\"O\"] + [f\"B-{e}\" for e in entity_types] + [f\"I-{e}\" for e in entity_types]\n",
    "    return BIO_LABELS_CACHE[key]\n",
    "\n",
    "def read_span_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=';', engine=\"python\", dtype=str, keep_default_na=False)\n",
    "    except Exception:\n",
    "        df = None\n",
    "        for sep in [\";\", \"|\", \"\\t\", \",\"]:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=sep, engine=\"python\", dtype=str, keep_default_na=False); break\n",
    "            except Exception:\n",
    "                pass\n",
    "        if df is None:\n",
    "            rows=[]\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line=line.rstrip(\"\\n\")\n",
    "                    for sep in [\"\\t\",\";\",\"|\",\",\"]:\n",
    "                        if sep in line:\n",
    "                            a,b = line.split(sep,1); rows.append({\"sample\":a,\"annotation\":b}); break\n",
    "                    else:\n",
    "                        rows.append({\"sample\":line,\"annotation\":\"[]\"})\n",
    "            df = pd.DataFrame(rows)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    s_col = cols.get(\"sample\", list(df.columns)[0])\n",
    "    a_col = cols.get(\"annotation\", list(df.columns)[1] if len(df.columns)>1 else None)\n",
    "    if a_col is None: raise ValueError(f\"{path}: need sample;annotation\")\n",
    "    df = df[[s_col,a_col]].rename(columns={s_col:\"sample\", a_col:\"annotation\"}).astype(str)\n",
    "    def parse_ann(s):\n",
    "        s=(s or \"\").strip()\n",
    "        if not s or s.lower()==\"nan\": return []\n",
    "        try:\n",
    "            arr = ast.literal_eval(s.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"'))\n",
    "            return [(int(a),int(b),str(t)) for (a,b,t) in arr]\n",
    "        except Exception: return []\n",
    "    df[\"annotation\"]=df[\"annotation\"].apply(parse_ann)\n",
    "    return df\n",
    "\n",
    "def split_with_offsets(text: str):\n",
    "    words=[]; i=0; n=len(text)\n",
    "    while i<n:\n",
    "        if text[i].isspace(): i+=1; continue\n",
    "        s=i\n",
    "        while i<n and not text[i].isspace(): i+=1\n",
    "        e=i; words.append((text[s:e], s, e))\n",
    "    return words\n",
    "\n",
    "def char_spans_to_bio_words(text: str, spans, ENTITY_TYPES, BIO_LABELS):\n",
    "    words = split_with_offsets(text)\n",
    "    labels = [\"O\"]*len(words)\n",
    "    norm=[]\n",
    "    for (s,e,tag) in spans:\n",
    "        tag=str(tag)\n",
    "        if tag==\"O\": continue\n",
    "        if \"-\" in tag:\n",
    "            bi,et = tag.split(\"-\",1); et=et.upper(); bi=\"B\" if bi not in (\"B\",\"I\") else bi\n",
    "            if et in ENTITY_TYPES: norm.append((s,e,bi,et))\n",
    "        else:\n",
    "            et=tag.upper()\n",
    "            if et in ENTITY_TYPES: norm.append((s,e,\"B\",et))\n",
    "    norm.sort(key=lambda x:(x[0],x[1]))\n",
    "    for (s,e,bi,et) in norm:\n",
    "        hits=[]\n",
    "        for wi,(_,ws,we) in enumerate(words):\n",
    "            if not (we<=s or ws>=e): hits.append(wi)\n",
    "        if not hits: continue\n",
    "        labels[hits[0]] = f\"B-{et}\"\n",
    "        for wi in hits[1:]: labels[wi] = f\"I-{et}\"\n",
    "    return [w for (w,_,__) in words], [l if l in BIO_LABELS else \"O\" for l in labels]\n",
    "\n",
    "def to_sequences(df: pd.DataFrame, source: str, ENTITY_TYPES=None, BIO_LABELS=None):\n",
    "    rows=[]\n",
    "    for _,r in df.iterrows():\n",
    "        toks, bio = char_spans_to_bio_words(r[\"sample\"], r[\"annotation\"], ENTITY_TYPES, BIO_LABELS)\n",
    "        rows.append({\"tokens\":toks, \"labels\":bio, \"sample\":r[\"sample\"], \"source\":source})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def text_group(text: str) -> str:\n",
    "    t=''.join(ch.lower() for ch in text if ch.isalpha() or ch.isspace())\n",
    "    return hashlib.md5(t.encode()).hexdigest()[:8]\n",
    "\n",
    "def group_split(df: pd.DataFrame, frac: float, seed: int):\n",
    "    if len(df)==0 or frac<=0: return df.index.values, np.array([],dtype=int)\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=frac, random_state=seed)\n",
    "    tr, va = next(gss.split(df, groups=df[\"group\"]))\n",
    "    return tr, va\n",
    "\n",
    "def compute_token_freqs(train_enc, n_labels: int):\n",
    "    cnt=Counter()\n",
    "    for row in train_enc:\n",
    "        for l in row[\"labels\"]:\n",
    "            if l != -100: cnt[int(l)] += 1\n",
    "    total = sum(cnt.values()) if cnt else 1\n",
    "    freqs = np.ones(n_labels, dtype=np.float64) / n_labels\n",
    "    for i in range(n_labels):\n",
    "        if i in cnt: freqs[i] = max(cnt[i]/total, 1e-8)\n",
    "    return freqs\n",
    "\n",
    "def bio_to_entities(seq):\n",
    "    ents=[]; cur=None; start=None\n",
    "    for i,tag in enumerate(seq+[\"O\"]):\n",
    "        if not isinstance(tag,str) or \"-\" not in tag:\n",
    "            if cur is not None: ents.append((cur,start,i)); cur=None; start=None\n",
    "            continue\n",
    "        bi,et = tag.split(\"-\",1)\n",
    "        if bi==\"B\":\n",
    "            if cur is not None: ents.append((cur,start,i))\n",
    "            cur, start = et, i\n",
    "        elif bi==\"I\":\n",
    "            if cur!=et:\n",
    "                if cur is not None: ents.append((cur,start,i))\n",
    "                cur, start = et, i\n",
    "        else:\n",
    "            if cur is not None: ents.append((cur,start,i))\n",
    "            cur=None; start=None\n",
    "    return ents\n",
    "\n",
    "def prf_by_type(true_bio, pred_bio, entity_types):\n",
    "    tp=defaultdict(int); fp=defaultdict(int); fn=defaultdict(int)\n",
    "    for y_t, y_p in zip(true_bio, y_pred := pred_bio):\n",
    "        ts={(t,s,e) for (t,s,e) in bio_to_entities(y_t) if t in entity_types}\n",
    "        ps={(t,s,e) for (t,s,e) in bio_to_entities(y_p) if t in entity_types}\n",
    "        for et in entity_types:\n",
    "            T={x for x in ts if x[0]==et}; P={x for x in ps if x[0]==et}\n",
    "            tp[et]+=len(T & P); fp[et]+=len(P-T); fn[et]+=len(T-P)\n",
    "    rows=[]\n",
    "    for et in entity_types:\n",
    "        T,Fp,Fn = tp[et], fp[et], fn[et]\n",
    "        prec = T/(T+Fp) if T+Fp>0 else 0.0\n",
    "        rec  = T/(T+Fn) if T+Fn>0 else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "        rows.append((et,T,Fp,Fn,prec,rec,f1))\n",
    "    macro_f1 = float(np.mean([r[6] for r in rows])) if rows else 0.0\n",
    "    return rows, macro_f1\n",
    "\n",
    "def make_compute_metrics(id2label: Dict[int,str], entity_types: List[str]):\n",
    "    def compute_metrics(p):\n",
    "        preds = p[0] if isinstance(p, tuple) else p.predictions\n",
    "        labels= p[1] if isinstance(p, tuple) else p.label_ids\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "        y_true, y_pred = [], []\n",
    "        for pr, lb in zip(preds, labels):\n",
    "            t,p = [],[]\n",
    "            for pi, li in zip(pr, lb):\n",
    "                if li == -100: continue\n",
    "                t.append(id2label[int(li)]); p.append(id2label[int(pi)])\n",
    "            y_true.append(t); y_pred.append(p)\n",
    "        res = {\n",
    "            \"precision_token\": precision_score(y_true, y_pred),\n",
    "            \"recall_token\":    recall_score(y_true, y_pred),\n",
    "            \"f1_token\":        f1_score(y_true, y_pred),\n",
    "            \"accuracy_token\":  accuracy_score(y_true, y_pred),\n",
    "        }\n",
    "        rows, macro_f1 = prf_by_type(y_true, y_pred, entity_types)\n",
    "        res[\"macro_f1_entity\"] = macro_f1\n",
    "        header = \"Entity-level (STRICT spans):\\nType       TP   FP   FN   Prec     Rec      F1\"\n",
    "        lines=[header]+[f\"{et:<10} {tp_:<4} {fp_:<4} {fn_:<4} {pr:>6.3f}  {rc:>6.3f}  {f1v:>6.3f}\"\n",
    "                        for (et,tp_,fp_,fn_,pr,rc,f1v) in rows]\n",
    "        lines.append(f\"\\nMacro-F1 (over {', '.join(entity_types)}): {macro_f1:.4f}\")\n",
    "        res[\"entity_report_text\"] = \"\\n\".join(lines)\n",
    "        return res\n",
    "    return compute_metrics\n",
    "\n",
    "def samplewise_label_counts(train_sequences_df: pd.DataFrame, label2id: Dict[str,int]) -> Counter:\n",
    "    c = Counter()\n",
    "    for _, row in train_sequences_df.iterrows():\n",
    "        seen = set(l for l in row[\"labels\"] if l in label2id)\n",
    "        for lab in seen:\n",
    "            c[label2id[lab]] += 1\n",
    "    return c\n",
    "\n",
    "def compute_label_weights_samplewise(train_sequences_df: pd.DataFrame, label2id: Dict[str,int],\n",
    "                                     min_w: float, max_w: float) -> torch.Tensor:\n",
    "    n_samples = max(len(train_sequences_df), 1)\n",
    "    counts = samplewise_label_counts(train_sequences_df, label2id)\n",
    "    w = torch.ones(len(label2id), dtype=torch.float32)\n",
    "    for lab, idx in label2id.items():\n",
    "        k = counts.get(idx, 0)\n",
    "        if k == 0:\n",
    "            w[idx] = max_w\n",
    "        else:\n",
    "            freq = k / n_samples\n",
    "            inv  = 1.0 / max(freq, 1e-8)\n",
    "            w[idx] = float(min(max_w, max(min_w, inv)))\n",
    "    w = w / w.mean()\n",
    "    return w\n",
    "\n",
    "class WeightedSmoothedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, label_smoothing=0.0, **kwargs):\n",
    "        self._class_weights = class_weights\n",
    "        self._label_smoothing = float(label_smoothing or 0.0)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._ce = None\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs); logits=outputs.logits\n",
    "        if self._ce is None:\n",
    "            cw = self._class_weights.to(logits.device) if self._class_weights is not None else None\n",
    "            try:\n",
    "                self._ce = nn.CrossEntropyLoss(weight=cw, ignore_index=-100, label_smoothing=self._label_smoothing)\n",
    "            except TypeError:\n",
    "                self._ce = nn.CrossEntropyLoss(weight=cw, ignore_index=-100)\n",
    "        loss = self._ce(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def build_training_args(args_over: Dict[str,Any], output_dir: str, grad_norm_clip: float, epochs: int, lr: float, batch: int, seed: int):\n",
    "    sig = inspect.signature(TrainingArguments.__init__).parameters\n",
    "    def has(k): return k in sig\n",
    "    args = dict(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        seed=seed,\n",
    "        gradient_accumulation_steps=1,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    if has(\"per_device_train_batch_size\"): args[\"per_device_train_batch_size\"]=batch\n",
    "    if has(\"per_device_eval_batch_size\"):  args[\"per_device_eval_batch_size\"]=batch\n",
    "    if has(\"evaluation_strategy\"): args[\"evaluation_strategy\"] = \"epoch\"\n",
    "    if has(\"eval_strategy\"):       args[\"eval_strategy\"]       = \"epoch\"\n",
    "    if has(\"save_strategy\"):       args[\"save_strategy\"]       = \"epoch\"\n",
    "    if has(\"logging_strategy\"):    args[\"logging_strategy\"]    = \"steps\"\n",
    "    if has(\"save_total_limit\"):         args[\"save_total_limit\"]=2\n",
    "    if has(\"load_best_model_at_end\"):   args[\"load_best_model_at_end\"]=True\n",
    "    if has(\"metric_for_best_model\"):    args[\"metric_for_best_model\"]=\"macro_f1_entity\"\n",
    "    if has(\"greater_is_better\"):        args[\"greater_is_better\"]=True\n",
    "    if has(\"warmup_ratio\"):             args[\"warmup_ratio\"]=0.1\n",
    "    if has(\"report_to\"):                args[\"report_to\"]=\"none\"\n",
    "    if has(\"max_grad_norm\"):            args[\"max_grad_norm\"]=grad_norm_clip\n",
    "    args.update(args_over or {})\n",
    "    return TrainingArguments(**args)\n",
    "\n",
    "def tokenize_and_align_factory(tokenizer, label2id, max_len: int):\n",
    "    def tokenize_and_align(examples):\n",
    "        enc = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True, max_length=max_len)\n",
    "        new_labels=[]\n",
    "        for i,labs in enumerate(examples[\"labels\"]):\n",
    "            word_ids = enc.word_ids(batch_index=i)\n",
    "            prev=None; ids=[]\n",
    "            for wi in word_ids:\n",
    "                if wi is None:\n",
    "                    ids.append(-100)\n",
    "                else:\n",
    "                    if wi!=prev:\n",
    "                        lab = labs[wi] if wi < len(labs) else \"O\"\n",
    "                        ids.append(label2id.get(lab, label2id[\"O\"]))\n",
    "                    else:\n",
    "                        ids.append(-100)\n",
    "                    prev=wi\n",
    "            new_labels.append(ids)\n",
    "        enc[\"labels\"]=new_labels\n",
    "        return enc\n",
    "    return tokenize_and_align\n",
    "\n",
    "def read_submission(path: str) -> pd.DataFrame:\n",
    "    df = None\n",
    "    for sep in [\";\", \",\", \"\\t\", \"|\"]:\n",
    "        try:\n",
    "            tmp = pd.read_csv(path, sep=sep, engine=\"python\", dtype=str, keep_default_na=False)\n",
    "            if len(tmp) and tmp.shape[1] >= 1:\n",
    "                df = tmp\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if df is None:\n",
    "        raise ValueError(f\"Не удалось прочитать {path} стандартными разделителями.\")\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    sample_col = lower.get(\"sample\", df.columns[0])\n",
    "    out = df[[sample_col]].rename(columns={sample_col: \"sample\"}).copy()\n",
    "    out[\"sample\"] = out[\"sample\"].astype(str)\n",
    "    return out\n",
    "\n",
    "def predict_bio_for_texts(model, tokenizer, id2label: Dict[int,str], texts: List[str], max_len: int, batch_size: int, device: str):\n",
    "    def split_with_offsets_local(text: str) -> List[Tuple[str, int, int]]:\n",
    "        words, i, n = [], 0, len(text)\n",
    "        while i < n:\n",
    "            if text[i].isspace():\n",
    "                i += 1\n",
    "                continue\n",
    "            s = i\n",
    "            while i < n and not text[i].isspace():\n",
    "                i += 1\n",
    "            e = i\n",
    "            words.append((text[s:e], s, e))\n",
    "        return words\n",
    "\n",
    "    all_labels, i = [], 0\n",
    "    model.eval()\n",
    "    while i < len(texts):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_words = [[w for (w, _, __) in split_with_offsets_local(t)] for t in batch_texts]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch_words,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        word_ids_batches = [enc.word_ids(batch_index=j) for j in range(len(batch_texts))]\n",
    "        enc_inputs = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc_inputs)\n",
    "            pred_ids = out.logits.argmax(-1).cpu().numpy()\n",
    "\n",
    "        for j in range(len(batch_texts)):\n",
    "            wi_list = word_ids_batches[j]\n",
    "            first_label_by_word = {}\n",
    "            for pos, wi in enumerate(wi_list):\n",
    "                if wi is None: continue\n",
    "                if wi not in first_label_by_word:\n",
    "                    first_label_by_word[wi] = int(pred_ids[j, pos])\n",
    "            num_words = len(batch_words[j])\n",
    "            pred_labels = [id2label.get(first_label_by_word.get(widx, 0), \"O\") for widx in range(num_words)]\n",
    "            all_labels.append(pred_labels)\n",
    "        i += batch_size\n",
    "    return all_labels\n",
    "\n",
    "def bio_words_to_char_spans(text: str, bio_labels: List[str]) -> List[Tuple[int,int,str]]:\n",
    "    words = split_with_offsets(text)\n",
    "    spans = []\n",
    "    for k, (_, s, e) in enumerate(words):\n",
    "        lab = bio_labels[k] if k < len(bio_labels) else \"O\"\n",
    "        spans.append((int(s), int(e), str(lab)))\n",
    "    return spans\n",
    "\n",
    "def fix_bi_chains(ann_str: str) -> str:\n",
    "    B_PREFIX = \"B-\"; I_PREFIX = \"I-\"\n",
    "    try:\n",
    "        spans = ast.literal_eval(ann_str)\n",
    "    except Exception:\n",
    "        return ann_str\n",
    "    if not isinstance(spans, list):\n",
    "        return ann_str\n",
    "    new_spans = []\n",
    "    prev_type = None\n",
    "    prev_is_entity = False\n",
    "    for (s, e, lab) in spans:\n",
    "        if not isinstance(lab, str):\n",
    "            new_spans.append((int(s), int(e), lab))\n",
    "            prev_type, prev_is_entity = None, False\n",
    "            continue\n",
    "        if lab == \"O\":\n",
    "            new_spans.append((int(s), int(e), lab))\n",
    "            prev_type, prev_is_entity = None, False\n",
    "            continue\n",
    "        if lab.startswith(B_PREFIX):\n",
    "            cur_type = lab[len(B_PREFIX):]\n",
    "            if prev_is_entity and prev_type == cur_type:\n",
    "                lab = f\"{I_PREFIX}{cur_type}\"\n",
    "            prev_type, prev_is_entity = cur_type, True\n",
    "            new_spans.append((int(s), int(e), lab))\n",
    "            continue\n",
    "        if lab.startswith(I_PREFIX):\n",
    "            cur_type = lab[len(I_PREFIX):]\n",
    "            prev_type, prev_is_entity = cur_type, True\n",
    "            new_spans.append((int(s), int(e), lab))\n",
    "            continue\n",
    "        new_spans.append((int(s), int(e), lab))\n",
    "        prev_type, prev_is_entity = None, False\n",
    "    return \"[\" + \", \".join(f\"({s}, {e}, {repr(l)})\" for (s, e, l) in new_spans) + \"]\"\n",
    "\n",
    "# ======================== ONE RUN ========================\n",
    "def run_single(hp: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    \"\"\"Train, evaluate, save model, produce submission; return artifacts + metrics.\"\"\"\n",
    "    cfg = DEFAULTS.copy()\n",
    "    cfg.update(hp or {})\n",
    "    set_all_seeds(cfg[\"SEED\"])\n",
    "\n",
    "    ENTITY_TYPES = cfg[\"ENTITY_TYPES\"]\n",
    "    BIO_LABELS   = label_set(ENTITY_TYPES)\n",
    "\n",
    "    # Paths and names\n",
    "    safe_mkdir(cfg[\"OUTPUT_ROOT\"])\n",
    "    safe_mkdir(cfg[\"SUBMIT_DIR\"])\n",
    "    tag = (\n",
    "        f\"lr{cfg['LR']}_bs{cfg['BATCH']}_ep{cfg['EPOCHS']}\"\n",
    "        f\"_ls{cfg['LABEL_SMOOTHING']}_do{cfg['DROPOUT_H']}-{cfg['DROPOUT_A']}\"\n",
    "        f\"_ml{cfg['MAX_LEN']}_seed{cfg['SEED']}\"\n",
    "    ).replace(\".\", \"p\")\n",
    "    run_dir = os.path.join(cfg[\"OUTPUT_ROOT\"], f\"{now_tag()}_{tag}\")\n",
    "    safe_mkdir(run_dir)\n",
    "\n",
    "    # ----- Load data\n",
    "    noisy_raw = read_span_csv(cfg[\"NOISY_PATH\"])\n",
    "    clean_raw = read_span_csv(cfg[\"CLEAN_PATH\"])\n",
    "\n",
    "    noisy_df  = to_sequences(noisy_raw, \"noisy\", ENTITY_TYPES, BIO_LABELS)\n",
    "    clean_df  = to_sequences(clean_raw, \"clean\", ENTITY_TYPES, BIO_LABELS)\n",
    "    noisy_df[\"group\"]=noisy_raw[\"sample\"].apply(text_group).values\n",
    "    clean_df[\"group\"]=clean_raw[\"sample\"].apply(text_group).values\n",
    "\n",
    "    n_tr, n_val = group_split(noisy_df, cfg[\"VAL_NOISY_FRAC\"], cfg[\"SEED\"])\n",
    "    c_tr, c_val = group_split(clean_df, cfg[\"VAL_CLEAN_FRAC\"], cfg[\"SEED\"])\n",
    "\n",
    "    if cfg[\"ENFORCE_VAL_MIX\"] and len(n_val)>0:\n",
    "        target_clean = int(round(len(n_val) * (1 - cfg[\"VAL_TARGET_NOISY_RATIO\"]) / max(cfg[\"VAL_TARGET_NOISY_RATIO\"],1e-9)))\n",
    "        if target_clean == 0 and len(c_val)>0:\n",
    "            target_clean = min(max(1, len(c_val)//10), len(c_val))\n",
    "        if target_clean < len(c_val):\n",
    "            rng = np.random.default_rng(cfg[\"SEED\"])\n",
    "            c_val = rng.choice(c_val, size=target_clean, replace=False)\n",
    "\n",
    "    val_df   = pd.concat([noisy_df.iloc[n_val], clean_df.iloc[c_val]], ignore_index=True)\n",
    "    train_df = pd.concat([noisy_df.iloc[n_tr], clean_df.iloc[c_tr]], ignore_index=True)\n",
    "\n",
    "    # ----- Tokenize\n",
    "    label2id = {l:i for i,l in enumerate(BIO_LABELS)}\n",
    "    id2label = {i:l for i,l in enumerate(BIO_LABELS)}\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg[\"MODEL_NAME\"], use_fast=True)\n",
    "    tok_fn = tokenize_and_align_factory(tokenizer, label2id, cfg[\"MAX_LEN\"])\n",
    "\n",
    "    ds = DatasetDict({\n",
    "        \"train\":      Dataset.from_pandas(train_df.drop(columns=[\"group\"])),\n",
    "        \"validation\": Dataset.from_pandas(val_df.drop(columns=[\"group\"]))\n",
    "    })\n",
    "    tok = ds.map(tok_fn, batched=True, remove_columns=[\"tokens\",\"labels\",\"sample\",\"source\"])\n",
    "    collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    # ----- Model & config\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        cfg[\"MODEL_NAME\"],\n",
    "        num_labels=len(BIO_LABELS),\n",
    "        id2label=id2label, label2id=label2id,\n",
    "        hidden_dropout_prob=cfg[\"DROPOUT_H\"],\n",
    "        attention_probs_dropout_prob=cfg[\"DROPOUT_A\"]\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(cfg[\"MODEL_NAME\"], config=config)\n",
    "\n",
    "    # init classifier bias by train priors\n",
    "    train_freq = compute_token_freqs(tok[\"train\"], len(BIO_LABELS))\n",
    "    with torch.no_grad():\n",
    "        bias = torch.log(torch.tensor(train_freq, dtype=torch.float32))\n",
    "        model.classifier.bias.copy_(bias)\n",
    "\n",
    "    # ----- class weights\n",
    "    class_weights = compute_label_weights_samplewise(train_df[[\"tokens\",\"labels\"]], label2id,\n",
    "                                                     min_w=cfg[\"MIN_WEIGHT\"], max_w=cfg[\"MAX_WEIGHT\"])\n",
    "\n",
    "    # ----- metrics\n",
    "    compute_metrics = make_compute_metrics(id2label, ENTITY_TYPES)\n",
    "\n",
    "    # ----- args + trainer\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "    training_args = build_training_args(\n",
    "        args_over={},\n",
    "        output_dir=run_dir,\n",
    "        grad_norm_clip=cfg[\"GRAD_NORM_CLIP\"],\n",
    "        epochs=cfg[\"EPOCHS\"],\n",
    "        lr=cfg[\"LR\"],\n",
    "        batch=cfg[\"BATCH\"],\n",
    "        seed=cfg[\"SEED\"],\n",
    "    )\n",
    "    trainer = WeightedSmoothedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tok[\"train\"],\n",
    "        eval_dataset=tok[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights,\n",
    "        label_smoothing=cfg[\"LABEL_SMOOTHING\"],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # ----- train + eval\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    pref = \"eval_\" if any(k.startswith(\"eval_\") for k in metrics.keys()) else \"\"\n",
    "    macro = float(metrics.get(pref+\"macro_f1_entity\", 0.0))\n",
    "    f1tok = float(metrics.get(pref+\"f1_token\", 0.0))\n",
    "\n",
    "    # ----- save model (+ id2label/label2id for local load)\n",
    "    trainer.save_model(run_dir)\n",
    "    with open(os.path.join(run_dir, \"id2label.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({int(k): v for k, v in id2label.items()}, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(run_dir, \"label2id.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({k: int(v) for k, v in label2id.items()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # ======================== Inference → submission ========================\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device); model.eval()\n",
    "\n",
    "    # Guard MAX_LEN by model's positional limit\n",
    "    cfg_max = getattr(config, \"max_position_embeddings\", None)\n",
    "    if not isinstance(cfg_max, int) or cfg_max <= 0:\n",
    "        MAX_LEN_EVAL = min(cfg[\"MAX_LEN\"], 512)\n",
    "    else:\n",
    "        MAX_LEN_EVAL = min(cfg[\"MAX_LEN\"], cfg_max, 512)\n",
    "\n",
    "    sub_df = read_submission(cfg[\"SUBMISSION_PATH\"])\n",
    "    texts  = sub_df[\"sample\"].tolist()\n",
    "    pred_bio = predict_bio_for_texts(model, tokenizer, id2label, texts, MAX_LEN_EVAL, cfg[\"BATCH_INFER\"], device)\n",
    "    pred_spans = [bio_words_to_char_spans(t, labs) for t, labs in zip(texts, pred_bio)]\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        \"sample\": sub_df[\"sample\"],\n",
    "        \"annotation\": [\"[\" + \", \".join(f\"({s}, {e}, {repr(lab)})\" for (s, e, lab) in sp) + \"]\" for sp in pred_spans]\n",
    "    })\n",
    "    raw_path  = os.path.join(run_dir, \"submission-bert.csv\")\n",
    "    post_path = os.path.join(run_dir, \"submission-bert-post.csv\")\n",
    "    out_df.to_csv(raw_path, index=False, sep=\";\")\n",
    "\n",
    "    # Postprocess B→I chains\n",
    "    post_df = out_df.copy()\n",
    "    post_df[\"annotation\"] = post_df[\"annotation\"].apply(fix_bi_chains)\n",
    "    post_df.to_csv(post_path, index=False, sep=\";\")\n",
    "\n",
    "    # Copy to submissions with informative filename\n",
    "    short_macro = f\"{macro:.4f}\".replace(\".\", \"p\")\n",
    "    short_f1tok = f\"{f1tok:.4f}\".replace(\".\", \"p\")\n",
    "    submit_name = f\"submission_{now_tag()}_{tag}_macro{short_macro}_f1tok{short_f1tok}.csv\"\n",
    "    final_submit_path = os.path.join(cfg[\"SUBMIT_DIR\"], submit_name)\n",
    "    post_df.to_csv(final_submit_path, index=False, sep=\";\")\n",
    "\n",
    "    # Free GPU memory between runs\n",
    "    del trainer, model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return dict(\n",
    "        run_dir=run_dir,\n",
    "        raw_path=raw_path,\n",
    "        post_path=post_path,\n",
    "        final_submit_path=final_submit_path,\n",
    "        metrics=metrics,\n",
    "        macro_f1_entity=macro,\n",
    "        f1_token=f1tok,\n",
    "        params=cfg,\n",
    "        tag=tag,\n",
    "    )\n",
    "\n",
    "# ======================== SWEEP ========================\n",
    "def main():\n",
    "    safe_mkdir(DEFAULTS[\"OUTPUT_ROOT\"])\n",
    "    safe_mkdir(DEFAULTS[\"SUBMIT_DIR\"])\n",
    "    log_csv = os.path.join(DEFAULTS[\"SUBMIT_DIR\"], \"tune_log.csv\")\n",
    "\n",
    "    # Define your search space here (edit freely!)\n",
    "    # Keep it modest if running on a single GPU overnight.\n",
    "    SEARCH_SPACE = [\n",
    "    dict(\n",
    "        LR=1.65e-5,\n",
    "        BATCH=48,\n",
    "        EPOCHS=5,\n",
    "        LABEL_SMOOTHING=0.03,\n",
    "        DROPOUT_H=0.1,\n",
    "        DROPOUT_A=0.1,\n",
    "        MAX_LEN=128,\n",
    "        GRAD_NORM_CLIP=0.8,\n",
    "    )\n",
    "]\n",
    "\n",
    "    # Optionally, set different seeds per run:\n",
    "    # for i, s in enumerate(SEARCH_SPACE): s[\"SEED\"] = 42 + i\n",
    "\n",
    "    # Prepare log\n",
    "    if not os.path.isfile(log_csv):\n",
    "        pd.DataFrame(columns=[\n",
    "            \"timestamp\",\"tag\",\"run_dir\",\"submission_file\",\n",
    "            \"macro_f1_entity\",\"f1_token\",\"params_json\"\n",
    "        ]).to_csv(log_csv, index=False)\n",
    "\n",
    "    for i, hp in enumerate(SEARCH_SPACE, start=1):\n",
    "        print(f\"\\n========== RUN {i}/{len(SEARCH_SPACE)}: {hp} ==========\")\n",
    "        try:\n",
    "            res = run_single(hp)\n",
    "            row = {\n",
    "                \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                \"tag\": res[\"tag\"],\n",
    "                \"run_dir\": res[\"run_dir\"],\n",
    "                \"submission_file\": res[\"final_submit_path\"],\n",
    "                \"macro_f1_entity\": res.get(\"macro_f1_entity\", None),\n",
    "                \"f1_token\": res.get(\"f1_token\", None),\n",
    "                \"params_json\": json.dumps(hp, ensure_ascii=False),\n",
    "            }\n",
    "            # append\n",
    "            pd.DataFrame([row]).to_csv(log_csv, mode=\"a\", header=False, index=False)\n",
    "            print(f\"✔ Saved submission → {res['final_submit_path']}\")\n",
    "            print(f\"✔ Logged to        → {log_csv}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✖ Run failed: {hp}\\n{e}\")\n",
    "            # log failure too\n",
    "            row = {\n",
    "                \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "                \"tag\": f\"FAILED_{now_tag()}\",\n",
    "                \"run_dir\": \"\",\n",
    "                \"submission_file\": \"\",\n",
    "                \"macro_f1_entity\": None,\n",
    "                \"f1_token\": None,\n",
    "                \"params_json\": json.dumps({**hp, \"_error\": str(e)}, ensure_ascii=False),\n",
    "            }\n",
    "            pd.DataFrame([row]).to_csv(log_csv, mode=\"a\", header=False, index=False)\n",
    "            # attempt to continue next run\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb24edb-07d1-4216-91af-2be1c0cf27ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
